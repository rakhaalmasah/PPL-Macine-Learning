{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-s-zHIxXFlZ6"
   },
   "source": [
    "# **HYPERPARAMETER TUNING - GOOGLE COLAB CODE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-eCwRgFo5GU"
   },
   "source": [
    "# Installing Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LixHUPdVD0LC"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lS8WeqLro8wZ"
   },
   "source": [
    "# Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7crHtae-rqd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJyxD2pZpH-I"
   },
   "source": [
    "# Load and Display Combined Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTuMznhwBMtX"
   },
   "outputs": [],
   "source": [
    "data_gabungan = pd.read_csv('/content/data_gabungan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSLNOrIyAhyL"
   },
   "outputs": [],
   "source": [
    "data_gabungan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySL8ma56pSZy"
   },
   "source": [
    "# Define Features (X) and Targets (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbJi5RXjBHAB"
   },
   "outputs": [],
   "source": [
    "X = data_gabungan[['Close USDIDR', 'BI Rate', 'Inflasi']].values\n",
    "\n",
    "Y = data_gabungan[['Close CTRA', 'Close INDF', 'Close ASII', 'Close BSDE',\n",
    "                 'Close ICBP', 'Close KLBF', 'Close ITMG', 'Close JPFA',\n",
    "                 'Close TLKM', 'Close ULTJ', 'Close ACES', 'Close TSPC',\n",
    "                  'Close SMAR', 'Close SMSM', 'Close JRPT', 'Close DUTI',\n",
    "                   'Close EPMT', 'Close SMCB', 'Close PWON', 'Close JSMR']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s64VFZpqpUmQ"
   },
   "source": [
    "# Split Data into Training and Validation Sets and Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i8M3Ma0CBUDu"
   },
   "outputs": [],
   "source": [
    "# 80% Train, 20% Val\n",
    "total_ukuran_data = len(X)\n",
    "ukuran_data_pelatihan = int(0.80 * total_ukuran_data)\n",
    "\n",
    "X_Latih, Y_Latih = X[:ukuran_data_pelatihan], Y[:ukuran_data_pelatihan]\n",
    "X_Validasi, Y_Validasi = X[ukuran_data_pelatihan:], Y[ukuran_data_pelatihan:]\n",
    "\n",
    "skala_X = StandardScaler()\n",
    "X_Latih_Standarisasi = skala_X.fit_transform(X_Latih)\n",
    "X_Validasi_Standarisasi = skala_X.transform(X_Validasi)\n",
    "\n",
    "skala_Y = StandardScaler()\n",
    "Y_Latih_Standarisasi = skala_Y.fit_transform(Y_Latih)\n",
    "Y_Validasi_Standarisasi = skala_Y.transform(Y_Validasi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsLzRqkIpbj0"
   },
   "source": [
    "# Defining a Hyperparameter-Tuned Model Builder Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # Detect TPU\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.TPUStrategy(tpu)  # Set up TPU strategy\n",
    "    print(\"Running on TPU\")\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()  # Default strategy if TPU is not available\n",
    "    print(\"Running on GPU/CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxQr-YSjE-2j"
   },
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  with strategy.scope():\n",
    "      model = tf.keras.Sequential()\n",
    "      model.add(tf.keras.layers.Input(shape=(X_Latih.shape[1],)))\n",
    "      num_layers = hp.Int('num_layers', min_value=2, max_value=6, step=1)\n",
    "      for i in range(num_layers):\n",
    "        units = hp.Int(f'units_{i}', min_value=16, max_value=512, step=16)\n",
    "        regularizer = hp.Float(f'kernel_regularizer_{i}', min_value=1e-6, max_value=1e-3, sampling='log')\n",
    "\n",
    "\n",
    "        model.add(tf.keras.layers.Dense(units=units,\n",
    "                                        activation='relu',\n",
    "                                        kernel_regularizer=regularizers.l2(regularizer),\n",
    "                                        ))\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dropout(rate=hp.Float(f'dropout_{i}', 0.2, 0.6, step=0.1)))\n",
    "\n",
    "      model.add(tf.keras.layers.Dense(20, activation='linear'))\n",
    "      \n",
    "      optimizer_choice = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop', 'adamw'])\n",
    "\n",
    "      if optimizer_choice == 'adam':\n",
    "          hp_learning_rate = hp.Float('adam_learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "          optimizer = tf.keras.optimizers.Adam(learning_rate=hp_learning_rate)\n",
    "\n",
    "      elif optimizer_choice == 'sgd':\n",
    "          hp_learning_rate = hp.Float('sgd_learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "          hp_momentum = hp.Float('sgd_momentum', min_value=0.0, max_value=0.9, step=0.1)\n",
    "          optimizer = tf.keras.optimizers.SGD(learning_rate=hp_learning_rate, momentum=hp_momentum)\n",
    "\n",
    "      elif optimizer_choice == 'rmsprop':\n",
    "          hp_learning_rate = hp.Float('rmsprop_learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "          hp_rho = hp.Float('rmsprop_rho', min_value=0.7, max_value=0.99, step=0.05)\n",
    "          optimizer = tf.keras.optimizers.RMSprop(learning_rate=hp_learning_rate, rho=hp_rho)\n",
    "\n",
    "      elif optimizer_choice == 'adamw':\n",
    "          hp_learning_rate = hp.Float('adamw_learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "          hp_weight_decay = hp.Float('adamw_weight_decay', min_value=1e-6, max_value=1e-2, sampling='log')\n",
    "          optimizer = tf.keras.optimizers.AdamW(learning_rate=hp_learning_rate, weight_decay=hp_weight_decay)\n",
    "\n",
    "      model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddzXFZqKpfWo"
   },
   "source": [
    "# Initializing the Hyperband Tuner for Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8YNWLDpFCIA"
   },
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_loss',\n",
    "                     max_epochs=100,\n",
    "                     factor=2,\n",
    "                     directory='TunnerV3',\n",
    "                     project_name='stock_price_prediction',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy1n4wwRpi-l"
   },
   "source": [
    "# Setting Up Early Stopping Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hTINNqTFFmz"
   },
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                              patience=10,\n",
    "                                              restore_best_weights=True\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPHrCGPzpnlX"
   },
   "source": [
    "# Running Hyperparameter Tuning with the Hyperband Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_i3tYtC_B5rj"
   },
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    X_Latih_Standarisasi,\n",
    "    Y_Latih_Standarisasi,\n",
    "    validation_data=(X_Validasi_Standarisasi, Y_Validasi_Standarisasi),\n",
    "    epochs=1000,\n",
    "    callbacks=[stop_early],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pFW5mr3ptcG"
   },
   "source": [
    "# Retrieving and Displaying the Best Hyperparameters from Tuning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5jogTfIJJ4V"
   },
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8-a9jTcJNxh"
   },
   "outputs": [],
   "source": [
    "print(\"Best Hyperparameters:\")\n",
    "for hp_name, hp_value in best_hps.values.items():\n",
    "    print(f\"{hp_name}: {hp_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNde1SM1pvO0"
   },
   "source": [
    "# Building and Displaying the Summary of the Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BUNyryd8BWdU"
   },
   "outputs": [],
   "source": [
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mmPMB1b4x5a"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnQ_MIiSp-I9"
   },
   "source": [
    "# Setting Up Model Checkpointing to Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1DWT_RmtlWV"
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'best_model_v3.keras'\n",
    "checkpoint = ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True,\n",
    "                             mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hf3we_YAqBnp"
   },
   "source": [
    "# Training the Model with Checkpointing and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6fcSQbqtq6l"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_Latih_Standarisasi, Y_Latih_Standarisasi,\n",
    "                    validation_data=(X_Validasi_Standarisasi, Y_Validasi_Standarisasi),\n",
    "                    epochs=1000,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jN_MkZVmqG2S"
   },
   "source": [
    "# Plotting Training and Validation Loss Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmIqkVQftv-y"
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs_range, loss, label='Training Loss', color='blue')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss', color='red')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"train.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCK9QOxWqNPW"
   },
   "source": [
    "# Loading the Best Model from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_v5 = tf.keras.models.load_model(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_v5.evaluate(X_Validasi_Standarisasi, Y_Validasi_Standarisasi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_1 = tf.keras.models.load_model(\"/content/best_model_v1.keras\")\n",
    "best_model_2 = tf.keras.models.load_model(\"/content/best_model_v2.keras\")\n",
    "best_model_3 = tf.keras.models.load_model(\"/content/best_model_v3.keras\")\n",
    "\n",
    "print(best_model_1.evaluate(X_Validasi_Standarisasi, Y_Validasi_Standarisasi))\n",
    "print(best_model_2.evaluate(X_Validasi_Standarisasi, Y_Validasi_Standarisasi))\n",
    "print(best_model_3.evaluate(X_Validasi_Standarisasi, Y_Validasi_Standarisasi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_1 = best_model_1(X_Validasi_Standarisasi).numpy()\n",
    "Y_pred_2 = best_model_2(X_Validasi_Standarisasi).numpy()\n",
    "Y_pred_3 = best_model_3(X_Validasi_Standarisasi).numpy()\n",
    "Y_pred_1 = skala_Y.inverse_transform(Y_pred_1)\n",
    "Y_pred_2 = skala_Y.inverse_transform(Y_pred_2)\n",
    "Y_pred_3 = skala_Y.inverse_transform(Y_pred_3)\n",
    "Y_Validasi = skala_Y.inverse_transform(Y_Validasi_Standarisasi)\n",
    "\n",
    "num_stocks = Y_Validasi.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_stocks):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(Y_Validasi[:, i], label='Real Data')\n",
    "    plt.plot(Y_pred_1[:, i], label='Prediction 1')\n",
    "    plt.plot(Y_pred_2[:, i], label='Prediction 2')\n",
    "    plt.plot(Y_pred_3[:, i], label='Prediction 3')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.title(f'Stock {i+1} Price Prediction') \n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_all, axes_all = plt.subplots(num_stocks, 4, figsize=(24, num_stocks * 6), sharey=False)\n",
    "\n",
    "for i in range(num_stocks):\n",
    "    axes_all[i, 0].plot(Y_Validasi[:, i], label='Real Data', color='blue')\n",
    "    axes_all[i, 0].set_title(f'Stock {i+1}: Actual')\n",
    "    axes_all[i, 0].set_xlabel('Time')\n",
    "    axes_all[i, 0].set_ylabel('Stock Price')\n",
    "    axes_all[i, 0].grid(True)\n",
    "    axes_all[i, 0].legend()\n",
    "\n",
    "    axes_all[i, 1].plot(Y_pred_1[:, i], label='Prediction 1', color='green')\n",
    "    axes_all[i, 1].set_title(f'Stock {i+1}: Prediction 1')\n",
    "    axes_all[i, 1].set_xlabel('Time')\n",
    "    axes_all[i, 1].grid(True)\n",
    "    axes_all[i, 1].legend()\n",
    "\n",
    "    axes_all[i, 2].plot(Y_pred_2[:, i], label='Prediction 2', color='red')\n",
    "    axes_all[i, 2].set_title(f'Stock {i+1}: Prediction 2')\n",
    "    axes_all[i, 2].set_xlabel('Time')\n",
    "    axes_all[i, 2].grid(True)\n",
    "    axes_all[i, 2].legend()\n",
    "\n",
    "    axes_all[i, 3].plot(Y_pred_3[:, i], label='Prediction 3', color='orange')\n",
    "    axes_all[i, 3].set_title(f'Stock {i+1}: Prediction 3')\n",
    "    axes_all[i, 3].set_xlabel('Time')\n",
    "    axes_all[i, 3].grid(True)\n",
    "    axes_all[i, 3].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = \"/content/stock_predictions_combined.png\"\n",
    "plt.savefig(output_path, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BV00IIFqQgP"
   },
   "source": [
    "# Making Predictions on New Data and Displaying Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeaXBqYfyjSy"
   },
   "outputs": [],
   "source": [
    "data_baru = np.array([[15569.15, 6.25, 2.12]])\n",
    "data_baru_standarisasi = skala_X.transform(data_baru)\n",
    "\n",
    "prediksi_standarisasi = model_terbaik.predict(data_baru_standarisasi)\n",
    "prediksi = skala_Y.inverse_transform(prediksi_standarisasi)\n",
    "\n",
    "saham = ['Close CTRA', 'Close INDF', 'Close ASII', 'Close BSDE', 'Close ICBP',\n",
    "          'Close KLBF', 'Close ITMG', 'Close JPFA', 'Close TLKM', 'Close ULTJ','Close ACES', 'Close TSPC',\n",
    "          'Close SMAR', 'Close SMSM', 'Close JRPT', 'Close DUTI', 'Close EPMT', 'Close SMCB', 'Close PWON', 'Close JSMR']\n",
    "hasil_prediksi = dict(zip(saham, prediksi[0]))\n",
    "\n",
    "print(\"Prediksi Harga Saham Berdasarkan Data Baru:\")\n",
    "for stock, price in hasil_prediksi.items():\n",
    "    print(f\"{stock}: {price:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhTQ1ilCqUjr"
   },
   "source": [
    "# Saving Standard Scalers to Disk Using Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lN6URUQWE36V"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(skala_X, 'skala_X.pkl')\n",
    "joblib.dump(skala_Y, 'skala_Y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUL5Wr6oE6Gk"
   },
   "outputs": [],
   "source": [
    "X_mean = skala_X.mean_\n",
    "X_std_dev = skala_X.scale_\n",
    "Y_mean = skala_Y.mean_\n",
    "Y_std_dev = skala_Y.scale_\n",
    "print(\"Mean dari X (fitur):\", X_mean)\n",
    "print(\"Standard Deviation dari X (fitur):\", X_std_dev)\n",
    "print(\"Mean dari Y (target):\", Y_mean)\n",
    "print(\"Standard Deviation dari Y (target):\", Y_std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlkZco0iqWqa"
   },
   "source": [
    "# Exporting the Best Model to a Specified Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HifbCMz7HWwK"
   },
   "outputs": [],
   "source": [
    "model_path = \"/content/best_model_v3.keras\"\n",
    "loaded_model = tf.keras.models.load_model(model_path)\n",
    "export_path = \"/content/best_model_v3_savedmodel\"\n",
    "tf.saved_model.save(loaded_model, export_path)\n",
    "\n",
    "zip_filename = \"/content/best_model_v3.zip\"\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "  for root, _, files in os.walk(export_path):\n",
    "    for file in files:\n",
    "      zipf.write(os.path.join(root, file),\n",
    "                 os.path.relpath(os.path.join(root, file),\n",
    "                                 export_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorflowjs_converter --input_format=tf_saved_model /content/best_model_v3_savedmodel /content/tfjs_model\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_filename = \"/content/tfjs_model.zip\"\n",
    "with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "  for root, _, files in os.walk(\"/content/tfjs_model\"):\n",
    "    for file in files:\n",
    "      zipf.write(os.path.join(root, file),\n",
    "                 os.path.relpath(os.path.join(root, file),\n",
    "                                 \"/content/tfjs_model\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
